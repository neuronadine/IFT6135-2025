{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neuronadine/IFT6135-2025.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjA0Bz-u0Lei",
        "outputId": "a526158d-9590-46a9-d03a-edbb52642e4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IFT6135-2025'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 41 (delta 12), reused 12 (delta 8), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (41/41), 415.47 KiB | 1.59 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qFHMMDtSwuW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cdb6a8-25ad-4778-d584-6aa401661e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQZtTBI21Vn8",
        "outputId": "9cc1a11d-2a53-4cdf-e97b-87f3c7d4154a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py  main.ipynb  main.py\tmlpmixer.py  mlp.py  model_configs  resnet18.py  test.py  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oODLwt1QzgGa"
      },
      "outputs": [],
      "source": [
        "#@title Link your assignment folder & install requirements\n",
        "#@markdown Enter the path to the assignment folder in your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "folder = \"/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/\" #@param {type:\"string\"}\n",
        "!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n",
        "\n",
        "# Add the assignment folder to Python path\n",
        "if '/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/' not in sys.path:\n",
        "  sys.path.insert(0, '/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/')\n",
        "\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  warnings.warn('CUDA is not available.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > Param√®tres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch import optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from utils import seed_experiment, to_device, cross_entropy_loss, compute_accuracy\n",
        "from config import get_config_parser\n",
        "import json\n",
        "from mlp import MLP\n",
        "from resnet18 import ResNet18\n",
        "from mlpmixer import MLPMixer\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZy1J-0OroLg"
      },
      "source": [
        "# Local Test\n",
        "Before run the experiment, here are some local test cases you can run for sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wLEVxwLlroLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bad8adf-c17b-4899-bfbc-4134f2618a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_linear_attributes (test.TestLinear.test_linear_attributes) ... ok\n",
            "test_linear_forward (test.TestLinear.test_linear_forward) ... ok\n",
            "test_activation (test.TestMLP.test_activation) ... ok\n",
            "test_forward (test.TestMLP.test_forward) ... ok\n",
            "test_mlp (test.TestMLP.test_mlp) ... ok\n",
            "test_mixer_block (test.TestMLPMixer.test_mixer_block) ... ok\n",
            "test_mlpmixer (test.TestMLPMixer.test_mlpmixer) ... ok\n",
            "test_patch_emb (test.TestMLPMixer.test_patch_emb) ... ok\n",
            "test_basic_block (test.TestResNet.test_basic_block) ... ok\n",
            "test_basic_block2 (test.TestResNet.test_basic_block2) ... ok\n",
            "test_resnet (test.TestResNet.test_resnet) ... ok\n",
            "test_ce_loss (test.TestUtils.test_ce_loss) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 12 tests in 1.773s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=12 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import unittest\n",
        "import test\n",
        "suite = unittest.TestLoader().loadTestsFromModule(test)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PtvL_yKp3PW"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWiJme7XaLiR"
      },
      "source": [
        "Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main_entry()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularizations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YUrqebfCobD1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Arguments:\n",
        "  # Data\n",
        "  batch_size: int = 128\n",
        "  # Model\n",
        "  model: str = 'mlp'  # [mlp, resnet18, mlpmixer]\n",
        "  model_config: str = \"./model_configs/mlp.json\" # path to model config json file\n",
        "\n",
        "  # Optimization\n",
        "  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n",
        "  epochs: int = 15\n",
        "  lr: float = 1e-3\n",
        "  momentum: float = 0.9\n",
        "  weight_decay: float = 5e-4\n",
        "\n",
        "  # Experiment\n",
        "  logdir: str = '/content/assignment/logs'\n",
        "  seed: int = 42\n",
        "\n",
        "  # Miscellaneous\n",
        "  device: str = 'cuda'\n",
        "  visualize : bool = False\n",
        "  print_every: int = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g2rjoY-5phTY"
      },
      "outputs": [],
      "source": [
        "# Main code entry. Train the model and save the logs\n",
        "from main import train, evaluate\n",
        "def main_entry(args):\n",
        "    # Check for the device\n",
        "    if (args.device == \"cuda\") and not torch.cuda.is_available():\n",
        "        warnings.warn(\n",
        "            \"CUDA is not available, make that your environment is \"\n",
        "            \"running on GPU (e.g. in the Notebook Settings in Google Colab). \"\n",
        "            'Forcing device=\"cpu\".'\n",
        "        )\n",
        "        args.device = \"cpu\"\n",
        "\n",
        "    if args.device == \"cpu\":\n",
        "        warnings.warn(\n",
        "            \"You are about to run on CPU, and might run out of memory \"\n",
        "            \"shortly. You can try setting batch_size=1 to reduce memory usage.\"\n",
        "        )\n",
        "\n",
        "    # Seed the experiment, for repeatability\n",
        "    seed_experiment(args.seed)\n",
        "\n",
        "    test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "    # For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                        ])\n",
        "    # Loading the training dataset. We need to split it into a training and validation part\n",
        "    # We need to do a little trick because the validation set should not use the augmentation.\n",
        "    train_dataset = CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
        "    val_dataset = CIFAR10(root='./data', train=True, transform=test_transform, download=True)\n",
        "    train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "    _, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
        "\n",
        "    # Loading the test set\n",
        "    test_set = CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n",
        "\n",
        "    # Load model\n",
        "    print(f'Build model {args.model.upper()}...')\n",
        "    if args.model_config is not None:\n",
        "        print(f'Loading model config from {args.model_config}')\n",
        "        with open(args.model_config) as f:\n",
        "            model_config = json.load(f)\n",
        "    else:\n",
        "        raise ValueError('Please provide a model config json')\n",
        "    print(f'########## {args.model.upper()} CONFIG ################')\n",
        "    for key, val in model_config.items():\n",
        "        print(f'{key}:\\t{val}')\n",
        "    print('############################################')\n",
        "    model_cls = {'mlp': MLP, 'resnet18': ResNet18, 'mlpmixer': MLPMixer}[args.model]\n",
        "    model = model_cls(**model_config)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Optimizer\n",
        "    if args.optimizer == \"adamw\":\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "        )\n",
        "    elif args.optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    elif args.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "        )\n",
        "    elif args.optimizer == \"momentum\":\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.weight_decay,\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n",
        "        f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n",
        "    )\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accs, valid_accs = [], []\n",
        "    train_times, valid_times = [], []\n",
        "\n",
        "    # We define a set of data loaders that we can use for various purposes later.\n",
        "    train_dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "    valid_dataloader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n",
        "    for epoch in range(args.epochs):\n",
        "        tqdm.write(f\"====== Epoch {epoch} ======>\")\n",
        "        loss, acc, wall_time = train(epoch, model, train_dataloader, optimizer,args)\n",
        "        train_losses.append(loss)\n",
        "        train_accs.append(acc)\n",
        "        train_times.append(wall_time)\n",
        "\n",
        "        loss, acc, wall_time = evaluate(epoch, model, valid_dataloader,args)\n",
        "        valid_losses.append(loss)\n",
        "        valid_accs.append(acc)\n",
        "        valid_times.append(wall_time)\n",
        "\n",
        "    test_loss, test_acc, test_time = evaluate(\n",
        "        epoch, model, test_dataloader, args, mode=\"test\"\n",
        "    )\n",
        "    print(f\"===== Best validation Accuracy: {max(valid_accs):.3f} =====>\")\n",
        "\n",
        "    # Save log if logdir provided\n",
        "    if args.logdir is not None:\n",
        "        print(f'Writing training logs to {args.logdir}...')\n",
        "        os.makedirs(args.logdir, exist_ok=True)\n",
        "        with open(os.path.join(args.logdir, 'results.json'), 'w') as f:\n",
        "            f.write(json.dumps(\n",
        "                {\n",
        "                    \"train_losses\": train_losses,\n",
        "                    \"valid_losses\": valid_losses,\n",
        "                    \"train_accs\": train_accs,\n",
        "                    \"valid_accs\": valid_accs,\n",
        "                    \"test_loss\": test_loss,\n",
        "                    \"test_acc\": test_acc\n",
        "                },\n",
        "                indent=4,\n",
        "            ))\n",
        "\n",
        "        # Visualize\n",
        "        if args.visualize and args.model in ['resnet18', 'mlpmixer']:\n",
        "            model.visualize(args.logdir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6ZKEOkM37Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZyJPWO1ppcTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b966152-556b-4769-ba71-8ac2feb3827c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Build model MLP...\n",
            "Loading model config from /content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/model_configs/mlp.json\n",
            "########## MLP CONFIG ################\n",
            "input_size:\t3072\n",
            "hidden_sizes:\t[1024, 512, 64, 64]\n",
            "num_classes:\t10\n",
            "activation:\trelu\n",
            "############################################\n",
            "Initialized MLP model with 3709194 total parameters, of which 3709194 are learnable.\n",
            "====== Epoch 0 ======>\n",
            "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.46330\n",
            "[TRAIN] Epoch: 0, Iter: 80, Loss: 1.76476\n",
            "[TRAIN] Epoch: 0, Iter: 160, Loss: 1.95951\n",
            "[TRAIN] Epoch: 0, Iter: 240, Loss: 1.57365\n",
            "[TRAIN] Epoch: 0, Iter: 320, Loss: 1.47636\n",
            "== [TRAIN] Epoch: 0, Accuracy: 0.357 ==>\n",
            "[VAL] Epoch: 0, Iter: 0, Loss: 1.70149\n",
            "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.410 ===>\n",
            "[TEST] Epoch: 0, Iter: 0, Loss: 1.50513\n",
            "=== [TEST] Epoch: 0, Iter: 78, Accuracy: 0.419 ===>\n",
            "===== Best validation Accuracy: 0.410 =====>\n",
            "Writing training logs to /content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/exps/mlp_default...\n"
          ]
        }
      ],
      "source": [
        "# Example to run MLP with 15 epochs\n",
        "config = Arguments(model='mlp',\n",
        "                   model_config='/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/model_configs/mlp.json',\n",
        "                   epochs=1, logdir=\"/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/exps/mlp_default\")\n",
        "main_entry(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jvTcJmnN379y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "results = {lr: {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []} for lr in learning_rates}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training ResNet18 with learning rate {lr}...\")\n",
        "\n",
        "    # Run experiment with the given learning rate\n",
        "    config = Arguments(\n",
        "        model='resnet18',\n",
        "        model_config='/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/model_configs/resnet18.json',\n",
        "        epochs=10,\n",
        "        lr=lr,\n",
        "        logdir=f\"/content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/exps/resnet/{lr}\"\n",
        "    )\n",
        "\n",
        "    main_entry(config)"
      ],
      "metadata": {
        "id": "i4YlXuAB38Na",
        "outputId": "3baec3f9-8806-4aca-ff3b-e8a405e0f25b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ResNet18 with learning rate 0.1...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Build model RESNET18...\n",
            "Loading model config from /content/gdrive/MyDrive/IFT6135-2025/HW1_2025/assignment1_release/model_configs/resnet18.json\n",
            "########## RESNET18 CONFIG ################\n",
            "num_classes:\t10\n",
            "############################################\n",
            "Initialized RESNET18 model with 11173962 total parameters, of which 11173962 are learnable.\n",
            "====== Epoch 0 ======>\n",
            "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.35367\n",
            "[TRAIN] Epoch: 0, Iter: 80, Loss: 2.09046\n",
            "[TRAIN] Epoch: 0, Iter: 160, Loss: 2.08024\n",
            "[TRAIN] Epoch: 0, Iter: 240, Loss: 1.79001\n",
            "[TRAIN] Epoch: 0, Iter: 320, Loss: 1.72314\n",
            "== [TRAIN] Epoch: 0, Accuracy: 0.250 ==>\n",
            "[VAL] Epoch: 0, Iter: 0, Loss: 1.82019\n",
            "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.372 ===>\n",
            "====== Epoch 1 ======>\n",
            "[TRAIN] Epoch: 1, Iter: 0, Loss: 1.77814\n",
            "[TRAIN] Epoch: 1, Iter: 80, Loss: 1.58036\n",
            "[TRAIN] Epoch: 1, Iter: 160, Loss: 1.63225\n",
            "[TRAIN] Epoch: 1, Iter: 240, Loss: 1.58753\n",
            "[TRAIN] Epoch: 1, Iter: 320, Loss: 1.43766\n",
            "== [TRAIN] Epoch: 1, Accuracy: 0.402 ==>\n",
            "[VAL] Epoch: 1, Iter: 0, Loss: 1.71151\n",
            "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.416 ===>\n",
            "====== Epoch 2 ======>\n",
            "[TRAIN] Epoch: 2, Iter: 0, Loss: 1.62642\n",
            "[TRAIN] Epoch: 2, Iter: 80, Loss: 1.51232\n",
            "[TRAIN] Epoch: 2, Iter: 160, Loss: 1.55846\n",
            "[TRAIN] Epoch: 2, Iter: 240, Loss: 1.43319\n",
            "[TRAIN] Epoch: 2, Iter: 320, Loss: 1.33458\n",
            "== [TRAIN] Epoch: 2, Accuracy: 0.475 ==>\n",
            "[VAL] Epoch: 2, Iter: 0, Loss: 1.35705\n",
            "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.523 ===>\n",
            "====== Epoch 3 ======>\n",
            "[TRAIN] Epoch: 3, Iter: 0, Loss: 1.43576\n",
            "[TRAIN] Epoch: 3, Iter: 80, Loss: 1.19092\n",
            "[TRAIN] Epoch: 3, Iter: 160, Loss: 1.41734\n",
            "[TRAIN] Epoch: 3, Iter: 240, Loss: 1.13249\n",
            "[TRAIN] Epoch: 3, Iter: 320, Loss: 1.33166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "results = {lr: {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []} for lr in learning_rates}\n",
        "\n",
        "for lr in learning_rates:\n",
        "  # Load results from log files\n",
        "  with open(f\"exps/resnet/{lr}/results.json\") as f:\n",
        "    logs = json.load(f)\n",
        "\n",
        "  # Store results\n",
        "  results[lr]['train_loss'] = logs['train_losses']\n",
        "  results[lr]['val_loss'] = logs['valid_losses']\n",
        "  results[lr]['train_acc'] = logs['train_accs']\n",
        "  results[lr]['val_acc'] = logs['valid_accs']\n",
        "\n",
        "# Plot results\n",
        "epochs = range(1, 11)\n",
        "\n",
        "# Plot Training Loss\n",
        "plt.figure()\n",
        "for lr in learning_rates:\n",
        "  plt.plot(epochs, results[lr]['train_loss'], label=f'LR={lr}')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss vs Epochs (ResNet18, Adam)\")\n",
        "plt.savefig(\"resnet18_train_loss.png\")\n",
        "\n",
        "# Plot Validation Loss\n",
        "plt.figure()\n",
        "for lr in learning_rates:\n",
        "  plt.plot(epochs, results[lr]['val_loss'], label=f'LR={lr}')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation Loss vs Epochs (ResNet18, Adam)\")\n",
        "plt.savefig(\"resnet18_val_loss.png\")\n",
        "\n",
        "# Plot Training Accuracy\n",
        "plt.figure()\n",
        "for lr in learning_rates:\n",
        "    plt.plot(epochs, results[lr]['train_acc'], label=f'LR={lr}')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Accuracy vs Epochs (ResNet18, Adam)\")\n",
        "plt.savefig(\"resnet18_train_acc.png\")\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "plt.figure()\n",
        "for lr in learning_rates:\n",
        "    plt.plot(epochs, results[lr]['val_acc'], label=f'LR={lr}')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation Accuracy vs Epochs (ResNet18, Adam)\")\n",
        "plt.savefig(\"resnet18_val_acc.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NIDYTn2G5aD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2 (default, Feb 26 2020, 14:31:49) \n[GCC 6.3.0 20170516]"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "81c564fb939afe7b3f114d194e01dc23538f9aaa81b9a9b61cd5d8751a87bdce"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}